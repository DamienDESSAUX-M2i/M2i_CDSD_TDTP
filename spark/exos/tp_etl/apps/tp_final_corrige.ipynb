{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Final : Analyse de Logs - CORRIGÉ\n",
    "\n",
    "Solutions complètes du TP d'analyse de logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TP_LogAnalysis_Corrige\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2 : Parsing - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern regex pour Apache Combined Log\n",
    "APACHE_PATTERN = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+) \"([^\"]*)\" \"([^\"]*)\"'\n",
    "\n",
    "def parse_logs(df_raw):\n",
    "    \"\"\"Parse les logs Apache en DataFrame structuré\"\"\"\n",
    "    \n",
    "    # Extraction avec regex\n",
    "    df_parsed = df_raw.select(\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 1).alias('ip'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 4).alias('timestamp_str'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 5).alias('method'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 6).alias('path'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 8).alias('status'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 9).alias('size'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 10).alias('referer'),\n",
    "        F.regexp_extract('value', APACHE_PATTERN, 11).alias('user_agent')\n",
    "    )\n",
    "    \n",
    "    # Conversion des types\n",
    "    df_typed = df_parsed \\\n",
    "        .withColumn(\"timestamp\", \n",
    "            F.to_timestamp(\"timestamp_str\", \"dd/MMM/yyyy:HH:mm:ss Z\")) \\\n",
    "        .withColumn(\"status\", F.col(\"status\").cast(\"int\")) \\\n",
    "        .withColumn(\"size\", \n",
    "            F.when(F.col(\"size\") == \"-\", 0)\n",
    "             .otherwise(F.col(\"size\").cast(\"long\"))) \\\n",
    "        .withColumn(\"date\", F.to_date(\"timestamp\")) \\\n",
    "        .withColumn(\"hour\", F.hour(\"timestamp\")) \\\n",
    "        .drop(\"timestamp_str\")\n",
    "    \n",
    "    # Filtrer les lignes valides\n",
    "    df_valid = df_typed.filter(\n",
    "        (F.col(\"ip\").isNotNull()) & \n",
    "        (F.col(\"ip\") != \"\") &\n",
    "        (F.col(\"timestamp\").isNotNull()) &\n",
    "        (F.col(\"status\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    return df_valid\n",
    "\n",
    "# Application\n",
    "# logs_clean = parse_logs(logs_raw)\n",
    "# logs_clean.cache()\n",
    "# print(f\"Logs valides: {logs_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3 : Métriques - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df):\n",
    "    \"\"\"Calcule les métriques principales\"\"\"\n",
    "    \n",
    "    # Requêtes par heure\n",
    "    hourly = df.groupBy(\n",
    "        F.date_trunc(\"hour\", \"timestamp\").alias(\"hour\")\n",
    "    ).agg(\n",
    "        F.count(\"*\").alias(\"requests\"),\n",
    "        F.countDistinct(\"ip\").alias(\"unique_ips\"),\n",
    "        F.sum(\"size\").alias(\"bytes_sent\")\n",
    "    ).orderBy(\"hour\")\n",
    "    \n",
    "    # Distribution des status codes\n",
    "    total = df.count()\n",
    "    status_dist = df.groupBy(\"status\").agg(\n",
    "        F.count(\"*\").alias(\"count\")\n",
    "    ).withColumn(\n",
    "        \"percentage\", F.round(F.col(\"count\") / total * 100, 2)\n",
    "    ).orderBy(F.desc(\"count\"))\n",
    "    \n",
    "    # Top pages\n",
    "    top_pages = df.groupBy(\"path\").agg(\n",
    "        F.count(\"*\").alias(\"hits\"),\n",
    "        F.countDistinct(\"ip\").alias(\"unique_visitors\")\n",
    "    ).orderBy(F.desc(\"hits\")).limit(10)\n",
    "    \n",
    "    # Top IPs\n",
    "    top_ips = df.groupBy(\"ip\").agg(\n",
    "        F.count(\"*\").alias(\"requests\"),\n",
    "        F.countDistinct(\"path\").alias(\"unique_paths\")\n",
    "    ).orderBy(F.desc(\"requests\")).limit(10)\n",
    "    \n",
    "    return {\n",
    "        \"hourly\": hourly,\n",
    "        \"status_dist\": status_dist,\n",
    "        \"top_pages\": top_pages,\n",
    "        \"top_ips\": top_ips\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4 : Détection d'anomalies - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df):\n",
    "    \"\"\"Détecte les anomalies dans les logs\"\"\"\n",
    "    \n",
    "    # IPs suspectes\n",
    "    suspicious_ips = df.groupBy(\"ip\").agg(\n",
    "        F.count(\"*\").alias(\"total_requests\"),\n",
    "        F.sum(F.when(F.col(\"status\") >= 400, 1).otherwise(0)).alias(\"errors\")\n",
    "    ).withColumn(\n",
    "        \"error_rate\", F.round(F.col(\"errors\") / F.col(\"total_requests\") * 100, 2)\n",
    "    ).filter(\n",
    "        (F.col(\"total_requests\") > 500) | (F.col(\"error_rate\") > 50)\n",
    "    ).orderBy(F.desc(\"total_requests\"))\n",
    "    \n",
    "    # Tentatives malveillantes\n",
    "    malicious_paths = [\"/admin\", \"/.env\", \"/wp-admin\", \"/phpmyadmin\", \n",
    "                       \"/config\", \"/.git\", \"/backup\"]\n",
    "    \n",
    "    malicious_attempts = df.filter(\n",
    "        F.col(\"path\").isin(malicious_paths)\n",
    "    ).groupBy(\"ip\", \"path\").agg(\n",
    "        F.count(\"*\").alias(\"attempts\"),\n",
    "        F.collect_set(\"status\").alias(\"statuses\")\n",
    "    ).orderBy(F.desc(\"attempts\"))\n",
    "    \n",
    "    # Pics de trafic (avec window functions)\n",
    "    hourly_traffic = df.groupBy(\n",
    "        F.date_trunc(\"hour\", \"timestamp\").alias(\"hour\")\n",
    "    ).count()\n",
    "    \n",
    "    window_spec = Window.orderBy(\"hour\").rowsBetween(-6, 0)\n",
    "    \n",
    "    traffic_anomalies = hourly_traffic \\\n",
    "        .withColumn(\"moving_avg\", F.avg(\"count\").over(window_spec)) \\\n",
    "        .withColumn(\"moving_std\", F.stddev(\"count\").over(window_spec)) \\\n",
    "        .withColumn(\"is_anomaly\",\n",
    "            F.abs(F.col(\"count\") - F.col(\"moving_avg\")) > 2 * F.col(\"moving_std\")\n",
    "        ).filter(F.col(\"is_anomaly\"))\n",
    "    \n",
    "    # Erreurs 5xx par endpoint\n",
    "    server_errors = df.filter(\n",
    "        F.col(\"status\").between(500, 599)\n",
    "    ).groupBy(\"path\").agg(\n",
    "        F.count(\"*\").alias(\"error_count\")\n",
    "    ).orderBy(F.desc(\"error_count\")).limit(10)\n",
    "    \n",
    "    return {\n",
    "        \"suspicious_ips\": suspicious_ips,\n",
    "        \"malicious_attempts\": malicious_attempts,\n",
    "        \"traffic_anomalies\": traffic_anomalies,\n",
    "        \"server_errors\": server_errors\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 5 : Export - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(df, output_path):\n",
    "    \"\"\"Exporte les résultats en Parquet partitionné\"\"\"\n",
    "    \n",
    "    # Ajouter colonnes de partitionnement\n",
    "    df_export = df \\\n",
    "        .withColumn(\"year\", F.year(\"timestamp\")) \\\n",
    "        .withColumn(\"month\", F.month(\"timestamp\")) \\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"timestamp\"))\n",
    "    \n",
    "    # Repartitionner pour optimiser l'écriture\n",
    "    df_repartitioned = df_export.repartition(\"date\")\n",
    "    \n",
    "    # Écrire en Parquet partitionné\n",
    "    df_repartitioned.write \\\n",
    "        .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(f\"{output_path}/logs_processed\")\n",
    "    \n",
    "    print(f\"Données exportées vers {output_path}/logs_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_report(metrics, anomalies):\n",
    "    \"\"\"Crée un rapport de synthèse\"\"\"\n",
    "\n",
    "    \n",
    "    print(\"\\nTop 5 Pages:\")\n",
    "    metrics[\"top_pages\"].show(5)\n",
    "    \n",
    "    print(\"\\nDistribution des Codes HTTP:\")\n",
    "    metrics[\"status_dist\"].show()\n",
    "    \n",
    "    print(\"\\nIPs Suspectes:\")\n",
    "    anomalies[\"suspicious_ips\"].show()\n",
    "    \n",
    "    print(\"\\nTentatives Malveillantes:\")\n",
    "    anomalies[\"malicious_attempts\"].show(10)\n",
    "    \n",
    "    print(\"\\nEndpoints avec Erreurs 5xx:\")\n",
    "    anomalies[\"server_errors\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
