{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 1 - Exploration et chargement Spark\n",
    "\n",
    "**Objectif** : Charger et explorer les donnees de qualite de l'air avec PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import os\n",
    "\n",
    "# Chemins des donnees\n",
    "DATA_DIR = \"../data\"\n",
    "AIR_QUALITY_PATH = os.path.join(DATA_DIR, \"air_quality_raw.csv\")\n",
    "STATIONS_PATH = os.path.join(DATA_DIR, \"stations.csv\")\n",
    "WEATHER_PATH = os.path.join(DATA_DIR, \"weather_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creation de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.7\n",
      "Spark UI: http://host.docker.internal:4041\n"
     ]
    }
   ],
   "source": [
    "# Creer une session Spark locale\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TP Qualite Air - Exploration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduire les logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Chargement des donnees de qualite de l'air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes: 1,230,951\n",
      "Nombre de colonnes: 5\n"
     ]
    }
   ],
   "source": [
    "# Charger le CSV avec inference de schema\n",
    "df_air_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(AIR_QUALITY_PATH)\n",
    "\n",
    "print(f\"Nombre de lignes: {df_air_raw.count():,}\")\n",
    "print(f\"Nombre de colonnes: {len(df_air_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema infere:\n",
      "root\n",
      " |-- station_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- pollutant: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Afficher le schema infere\n",
    "print(\"Schema infere:\")\n",
    "df_air_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+-----+-----+\n",
      "|station_id|timestamp          |pollutant|value|unit |\n",
      "+----------+-------------------+---------+-----+-----+\n",
      "|ST0040    |2024-01-07T05:00:00|O3       |79.29|ug/m3|\n",
      "|ST0004    |06/09/2024 18:00:00|O3       |41.58|ug/m3|\n",
      "|ST0027    |2024-05-23 11:00:00|PM10     |29.20|ug/m3|\n",
      "|ST0002    |18/03/2024 12:00   |SO2      |7.72 |ug/m3|\n",
      "|ST0035    |2024-06-11T08:00:00|O3       |29.87|ug/m3|\n",
      "|ST0023    |19/04/2024 10:00   |O3       |30.07|ug/m3|\n",
      "|ST0035    |19/03/2024 05:00   |PM2.5    |14.12|ug/m3|\n",
      "|ST0001    |2024-05-19T13:00:00|PM10     |17,09|ug/m3|\n",
      "|ST0014    |10/03/2024 20:00   |CO       |0.29 |mg/m3|\n",
      "|ST0004    |2024-01-28T16:00:00|NO2      |25,69|ug/m3|\n",
      "+----------+-------------------+---------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apercu des donnees\n",
    "df_air_raw.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Identification des problemes de typage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs non numeriques: 6,076\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| null|\n",
      "|  N/A|\n",
      "|error|\n",
      "|  ---|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# La colonne 'value' est en string car elle contient des virgules et des valeurs textuelles\n",
    "# Examinons les valeurs non numeriques\n",
    "\n",
    "# Valeurs qui ne peuvent pas etre converties en nombre\n",
    "df_non_numeric = df_air_raw.filter(\n",
    "    ~F.col(\"value\").rlike(\"^-?[0-9]+[.,]?[0-9]*$\")\n",
    ")\n",
    "\n",
    "print(f\"Nombre de valeurs non numeriques: {df_non_numeric.count():,}\")\n",
    "df_non_numeric.select(\"value\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs avec virgule: 184,556\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|17,09|\n",
      "|25,69|\n",
      "|15,42|\n",
      "|29,14|\n",
      "|12,82|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Valeurs avec virgule comme separateur decimal\n",
    "df_with_comma = df_air_raw.filter(F.col(\"value\").contains(\",\"))\n",
    "print(f\"Nombre de valeurs avec virgule: {df_with_comma.count():,}\")\n",
    "df_with_comma.select(\"value\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemples de formats de timestamp:\n",
      "+-------------------+\n",
      "|timestamp          |\n",
      "+-------------------+\n",
      "|2024-01-07T05:00:00|\n",
      "|2024-05-19T13:00:00|\n",
      "|05/26/2024 14:00:00|\n",
      "|2024-04-12T09:00:00|\n",
      "|2024-01-31T14:00:00|\n",
      "|2024-06-02 20:00:00|\n",
      "|05/18/2024 07:00:00|\n",
      "|2024-04-26T22:00:00|\n",
      "|2024-03-31T02:00:00|\n",
      "|28/04/2024 22:00   |\n",
      "|2024-03-22 10:00:00|\n",
      "|2024-04-02 00:00:00|\n",
      "|2024-03-30 18:00:00|\n",
      "|16/03/2024 02:00   |\n",
      "|04/22/2024 22:00:00|\n",
      "|03/03/2024 02:00:00|\n",
      "|2024-06-28 11:00:00|\n",
      "|12/05/2024 12:00   |\n",
      "|17/01/2024 03:00   |\n",
      "|2024-03-28T08:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Differents formats de timestamp\n",
    "print(\"Exemples de formats de timestamp:\")\n",
    "df_air_raw.select(\"timestamp\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Statistiques descriptives par polluant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques par polluant:\n",
      "+---------+------+------+------+-------+-------+------+\n",
      "|pollutant| count|  mean|stddev|    min|    max|median|\n",
      "+---------+------+------+------+-------+-------+------+\n",
      "|       CO|204062| 33.71|341.16|  -2.57|4996.48|  0.71|\n",
      "|      NO2|204093| 78.09|336.51|-163.48|4998.69| 42.43|\n",
      "|       O3|204220|108.78|337.01|-269.95|4999.37| 70.52|\n",
      "|     PM10|204133|  70.1|335.05|-132.57|4999.15| 35.25|\n",
      "|    PM2.5|204189| 55.31|336.76| -81.23|4999.69| 21.18|\n",
      "|      SO2|204178| 40.73|342.75| -26.57|4997.95|  7.05|\n",
      "+---------+------+------+------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertir value en double (en remplacant la virgule par un point)\n",
    "df_air_numeric = df_air_raw.withColumn(\n",
    "    \"value_clean\",\n",
    "    F.regexp_replace(F.col(\"value\"), \",\", \".\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Statistiques par polluant (en ignorant les valeurs nulles)\n",
    "stats_by_pollutant = df_air_numeric.filter(F.col(\"value_clean\").isNotNull()) \\\n",
    "    .groupBy(\"pollutant\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.round(F.mean(\"value_clean\"), 2).alias(\"mean\"),\n",
    "        F.round(F.stddev(\"value_clean\"), 2).alias(\"stddev\"),\n",
    "        F.round(F.min(\"value_clean\"), 2).alias(\"min\"),\n",
    "        F.round(F.max(\"value_clean\"), 2).alias(\"max\"),\n",
    "        F.round(F.expr(\"percentile(value_clean, 0.5)\"), 2).alias(\"median\")\n",
    "    ) \\\n",
    "    .orderBy(\"pollutant\")\n",
    "\n",
    "print(\"Statistiques par polluant:\")\n",
    "stats_by_pollutant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs negatives:\n",
      "+---------+-----+\n",
      "|pollutant|count|\n",
      "+---------+-----+\n",
      "|      SO2| 2018|\n",
      "|       O3| 2062|\n",
      "|      NO2| 2033|\n",
      "|     PM10| 2040|\n",
      "|       CO| 2087|\n",
      "|    PM2.5| 2070|\n",
      "+---------+-----+\n",
      "\n",
      "\n",
      "Valeurs > 1000 ug/m3:\n",
      "+---------+-----+\n",
      "|pollutant|count|\n",
      "+---------+-----+\n",
      "|      SO2| 2066|\n",
      "|       O3| 2080|\n",
      "|      NO2| 2031|\n",
      "|     PM10| 2017|\n",
      "|       CO| 2070|\n",
      "|    PM2.5| 2063|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifier les valeurs aberrantes\n",
    "print(\"Valeurs negatives:\")\n",
    "df_air_numeric.filter(F.col(\"value_clean\") < 0).groupBy(\"pollutant\").count().show()\n",
    "\n",
    "print(\"\\nValeurs > 1000 ug/m3:\")\n",
    "df_air_numeric.filter(F.col(\"value_clean\") > 1000).groupBy(\"pollutant\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Comptage des valeurs nulles par colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs nulles/vides par colonne:\n",
      "+----------+---------+---------+-----+----+\n",
      "|station_id|timestamp|pollutant|value|unit|\n",
      "+----------+---------+---------+-----+----+\n",
      "|         0|        0|        0| 1538|   0|\n",
      "+----------+---------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compter les nulls pour chaque colonne\n",
    "null_counts = df_air_raw.select([\n",
    "    F.count(F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\") | (F.trim(F.lower(F.col(c))).isin(\"null\", \"none\")), c)).alias(c)\n",
    "    for c in df_air_raw.columns\n",
    "])\n",
    "\n",
    "print(\"Nombre de valeurs nulles/vides par colonne:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taux de completude (sur 1,230,951 lignes):\n",
      "  station_id: 100.00%\n",
      "  timestamp: 100.00%\n",
      "  pollutant: 100.00%\n",
      "  value: 99.88%\n",
      "  unit: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Pourcentage de completude\n",
    "total_rows = df_air_raw.count()\n",
    "print(f\"\\nTaux de completude (sur {total_rows:,} lignes):\")\n",
    "\n",
    "for col_name in df_air_raw.columns:\n",
    "    null_count = df_air_raw.filter(\n",
    "        F.col(col_name).isNull() | (F.trim(F.col(col_name)) == \"\") | (F.trim(F.lower(F.col(col_name))).isin(\"null\", \"none\"))\n",
    "    ).count()\n",
    "    completude = (1 - null_count / total_rows) * 100\n",
    "    print(f\"  {col_name}: {completude:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Stations avec le plus d'enregistrements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+---------+--------+------------+\n",
      "|station_id|        station_name|     city|      lat|     lon|station_type|\n",
      "+----------+--------------------+---------+---------+--------+------------+\n",
      "|    ST0001|     Paris-urbaine-1|    Paris|48.809101|2.329703|     urbaine|\n",
      "|    ST0002| Paris-periurbaine-2|    Paris|48.828921|2.375847| periurbaine|\n",
      "|    ST0003|Paris-industrielle-3|    Paris| 48.87427|2.391418|industrielle|\n",
      "|    ST0004|      Lyon-urbaine-1|     Lyon|45.773049|4.788878|     urbaine|\n",
      "|    ST0005|  Lyon-periurbaine-2|     Lyon| 45.72337|4.808966| periurbaine|\n",
      "|    ST0006| Lyon-industrielle-3|     Lyon|45.774202|4.841825|industrielle|\n",
      "|    ST0007| Marseille-urbaine-1|Marseille|43.288452|5.364721|     urbaine|\n",
      "|    ST0008|Marseille-periurb...|Marseille|43.274319| 5.40673| periurbaine|\n",
      "|    ST0009|Marseille-industr...|Marseille|43.322381|5.335766|industrielle|\n",
      "|    ST0010|  Marseille-trafic-4|Marseille|43.288761|5.347587|      trafic|\n",
      "+----------+--------------------+---------+---------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Charger les stations\n",
    "df_stations = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(STATIONS_PATH)\n",
    "\n",
    "df_stations.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 stations avec le plus d'enregistrements:\n",
      "+----------+--------------------+----------+------------+-----+\n",
      "|station_id|        station_name|      city|station_type|count|\n",
      "+----------+--------------------+----------+------------+-----+\n",
      "|    ST0019|  Bordeaux-urbaine-1|  Bordeaux|     urbaine|26164|\n",
      "|    ST0016|Toulouse-industri...|  Toulouse|industrielle|26172|\n",
      "|    ST0003|Paris-industrielle-3|     Paris|industrielle|26239|\n",
      "|    ST0007| Marseille-urbaine-1| Marseille|     urbaine|26219|\n",
      "|    ST0034| Strasbourg-trafic-4|Strasbourg|      trafic|26190|\n",
      "|    ST0017|   Toulouse-trafic-4|  Toulouse|      trafic|26199|\n",
      "|    ST0002| Paris-periurbaine-2|     Paris| periurbaine|26176|\n",
      "|    ST0035|  Grenoble-urbaine-1|  Grenoble|     urbaine|26221|\n",
      "|    ST0028|Nantes-periurbaine-2|    Nantes| periurbaine|26241|\n",
      "|    ST0037|Grenoble-industri...|  Grenoble|industrielle|26233|\n",
      "+----------+--------------------+----------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Nombre d'enregistrements par station\n",
    "records_by_station = df_air_raw.groupBy(\"station_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Joindre avec les infos des stations\n",
    "records_with_info = records_by_station.join(\n",
    "    df_stations,\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"station_id\", \"station_name\", \"city\", \"station_type\", \"count\"\n",
    ")\n",
    "\n",
    "print(\"Top 10 stations avec le plus d'enregistrements:\")\n",
    "records_with_info.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition des enregistrements par ville:\n",
      "+----------+------+\n",
      "|      city| count|\n",
      "+----------+------+\n",
      "|     Rouen|183348|\n",
      "| Marseille|183294|\n",
      "|  Grenoble|157073|\n",
      "|  Toulouse|130979|\n",
      "|     Lille|130897|\n",
      "|    Nantes|104840|\n",
      "|Strasbourg|104791|\n",
      "|     Paris| 78611|\n",
      "|  Bordeaux| 78574|\n",
      "|      Lyon| 78544|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repartition par ville\n",
    "records_by_city = df_air_raw.join(\n",
    "    df_stations.select(\"station_id\", \"city\"),\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ").groupBy(\"city\") \\\n",
    " .count() \\\n",
    " .orderBy(F.desc(\"count\"))\n",
    "\n",
    "print(\"Repartition des enregistrements par ville:\")\n",
    "records_by_city.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Synthese des problemes de qualite identifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total enregistrements: 1,230,951\n",
      "\n",
      "Problemes identifies:\n",
      "  - Valeurs non numeriques: 6,076 (0.49%)\n",
      "  - Valeurs avec virgule decimale: 184,556 (14.99%)\n",
      "  - Valeurs negatives: 12,310 (1.00%)\n",
      "  - Valeurs aberrantes (>1000): 12,327 (1.00%)\n",
      "  - Doublons: 24,136 (1.96%)\n",
      "  - Formats de dates multiples: 4 formats differents detectes\n"
     ]
    }
   ],
   "source": [
    "# Resume des problemes\n",
    "total = df_air_raw.count()\n",
    "\n",
    "# Valeurs non numeriques\n",
    "non_numeric = df_air_raw.filter(\n",
    "    ~F.col(\"value\").rlike(\"^-?[0-9]+[.,]?[0-9]*$\")\n",
    ").count()\n",
    "\n",
    "# Valeurs avec virgule\n",
    "with_comma = df_air_raw.filter(F.col(\"value\").contains(\",\")).count()\n",
    "\n",
    "# Valeurs negatives (apres conversion)\n",
    "negative = df_air_numeric.filter(F.col(\"value_clean\") < 0).count()\n",
    "\n",
    "# Valeurs aberrantes > 1000\n",
    "outliers = df_air_numeric.filter(F.col(\"value_clean\") > 1000).count()\n",
    "\n",
    "# Doublons\n",
    "duplicates = total - df_air_raw.dropDuplicates([\"station_id\", \"timestamp\", \"pollutant\"]).count()\n",
    "\n",
    "\n",
    "print(f\"Total enregistrements: {total:,}\")\n",
    "print()\n",
    "print(f\"Problemes identifies:\")\n",
    "print(f\"  - Valeurs non numeriques: {non_numeric:,} ({non_numeric/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs avec virgule decimale: {with_comma:,} ({with_comma/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs negatives: {negative:,} ({negative/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs aberrantes (>1000): {outliers:,} ({outliers/total*100:.2f}%)\")\n",
    "print(f\"  - Doublons: {duplicates:,} ({duplicates/total*100:.2f}%)\")\n",
    "print(f\"  - Formats de dates multiples: 4 formats differents detectes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer la session Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
