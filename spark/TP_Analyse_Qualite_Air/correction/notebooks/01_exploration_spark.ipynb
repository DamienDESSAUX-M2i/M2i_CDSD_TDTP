{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etape 1 - Exploration et chargement Spark\n",
    "\n",
    "**Objectif** : Charger et explorer les donnees de qualite de l'air avec PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "import os\n",
    "\n",
    "# Chemins des donnees\n",
    "DATA_DIR = \"../data\"\n",
    "AIR_QUALITY_PATH = os.path.join(DATA_DIR, \"air_quality_raw.csv\")\n",
    "STATIONS_PATH = os.path.join(DATA_DIR, \"stations.csv\")\n",
    "WEATHER_PATH = os.path.join(DATA_DIR, \"weather_raw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creation de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer une session Spark locale\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TP Qualite Air - Exploration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reduire les logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Chargement des donnees de qualite de l'air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le CSV avec inference de schema\n",
    "df_air_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(AIR_QUALITY_PATH)\n",
    "\n",
    "print(f\"Nombre de lignes: {df_air_raw.count():,}\")\n",
    "print(f\"Nombre de colonnes: {len(df_air_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le schema infere\n",
    "print(\"Schema infere:\")\n",
    "df_air_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apercu des donnees\n",
    "df_air_raw.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Identification des problemes de typage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La colonne 'value' est en string car elle contient des virgules et des valeurs textuelles\n",
    "# Examinons les valeurs non numeriques\n",
    "\n",
    "# Valeurs qui ne peuvent pas etre converties en nombre\n",
    "df_non_numeric = df_air_raw.filter(\n",
    "    ~F.col(\"value\").rlike(\"^-?[0-9]+[.,]?[0-9]*$\")\n",
    ")\n",
    "\n",
    "print(f\"Nombre de valeurs non numeriques: {df_non_numeric.count():,}\")\n",
    "df_non_numeric.select(\"value\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs avec virgule comme separateur decimal\n",
    "df_with_comma = df_air_raw.filter(F.col(\"value\").contains(\",\"))\n",
    "print(f\"Nombre de valeurs avec virgule: {df_with_comma.count():,}\")\n",
    "df_with_comma.select(\"value\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differents formats de timestamp\n",
    "print(\"Exemples de formats de timestamp:\")\n",
    "df_air_raw.select(\"timestamp\").distinct().show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Statistiques descriptives par polluant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir value en double (en remplacant la virgule par un point)\n",
    "df_air_numeric = df_air_raw.withColumn(\n",
    "    \"value_clean\",\n",
    "    F.regexp_replace(F.col(\"value\"), \",\", \".\").cast(\"double\")\n",
    ")\n",
    "\n",
    "# Statistiques par polluant (en ignorant les valeurs nulles)\n",
    "stats_by_pollutant = df_air_numeric.filter(F.col(\"value_clean\").isNotNull()) \\\n",
    "    .groupBy(\"pollutant\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.round(F.mean(\"value_clean\"), 2).alias(\"mean\"),\n",
    "        F.round(F.stddev(\"value_clean\"), 2).alias(\"stddev\"),\n",
    "        F.round(F.min(\"value_clean\"), 2).alias(\"min\"),\n",
    "        F.round(F.max(\"value_clean\"), 2).alias(\"max\"),\n",
    "        F.round(F.expr(\"percentile(value_clean, 0.5)\"), 2).alias(\"median\")\n",
    "    ) \\\n",
    "    .orderBy(\"pollutant\")\n",
    "\n",
    "print(\"Statistiques par polluant:\")\n",
    "stats_by_pollutant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les valeurs aberrantes\n",
    "print(\"Valeurs negatives:\")\n",
    "df_air_numeric.filter(F.col(\"value_clean\") < 0).groupBy(\"pollutant\").count().show()\n",
    "\n",
    "print(\"\\nValeurs > 1000 ug/m3:\")\n",
    "df_air_numeric.filter(F.col(\"value_clean\") > 1000).groupBy(\"pollutant\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Comptage des valeurs nulles par colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les nulls pour chaque colonne\n",
    "null_counts = df_air_raw.select([\n",
    "    F.count(F.when(F.col(c).isNull() | (F.col(c) == \"\"), c)).alias(c)\n",
    "    for c in df_air_raw.columns\n",
    "])\n",
    "\n",
    "print(\"Nombre de valeurs nulles/vides par colonne:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pourcentage de completude\n",
    "total_rows = df_air_raw.count()\n",
    "print(f\"\\nTaux de completude (sur {total_rows:,} lignes):\")\n",
    "\n",
    "for col_name in df_air_raw.columns:\n",
    "    null_count = df_air_raw.filter(\n",
    "        F.col(col_name).isNull() | (F.col(col_name) == \"\")\n",
    "    ).count()\n",
    "    completude = (1 - null_count / total_rows) * 100\n",
    "    print(f\"  {col_name}: {completude:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Stations avec le plus d'enregistrements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les stations\n",
    "df_stations = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(STATIONS_PATH)\n",
    "\n",
    "df_stations.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'enregistrements par station\n",
    "records_by_station = df_air_raw.groupBy(\"station_id\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Joindre avec les infos des stations\n",
    "records_with_info = records_by_station.join(\n",
    "    df_stations,\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    \"station_id\", \"station_name\", \"city\", \"station_type\", \"count\"\n",
    ")\n",
    "\n",
    "print(\"Top 10 stations avec le plus d'enregistrements:\")\n",
    "records_with_info.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition par ville\n",
    "records_by_city = df_air_raw.join(\n",
    "    df_stations.select(\"station_id\", \"city\"),\n",
    "    on=\"station_id\",\n",
    "    how=\"left\"\n",
    ").groupBy(\"city\") \\\n",
    " .count() \\\n",
    " .orderBy(F.desc(\"count\"))\n",
    "\n",
    "print(\"Repartition des enregistrements par ville:\")\n",
    "records_by_city.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Synthese des problemes de qualite identifies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume des problemes\n",
    "total = df_air_raw.count()\n",
    "\n",
    "# Valeurs non numeriques\n",
    "non_numeric = df_air_raw.filter(\n",
    "    ~F.col(\"value\").rlike(\"^-?[0-9]+[.,]?[0-9]*$\")\n",
    ").count()\n",
    "\n",
    "# Valeurs avec virgule\n",
    "with_comma = df_air_raw.filter(F.col(\"value\").contains(\",\")).count()\n",
    "\n",
    "# Valeurs negatives (apres conversion)\n",
    "negative = df_air_numeric.filter(F.col(\"value_clean\") < 0).count()\n",
    "\n",
    "# Valeurs aberrantes > 1000\n",
    "outliers = df_air_numeric.filter(F.col(\"value_clean\") > 1000).count()\n",
    "\n",
    "# Doublons\n",
    "duplicates = total - df_air_raw.dropDuplicates([\"station_id\", \"timestamp\", \"pollutant\"]).count()\n",
    "\n",
    "\n",
    "print(f\"Total enregistrements: {total:,}\")\n",
    "print()\n",
    "print(f\"Problemes identifies:\")\n",
    "print(f\"  - Valeurs non numeriques: {non_numeric:,} ({non_numeric/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs avec virgule decimale: {with_comma:,} ({with_comma/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs negatives: {negative:,} ({negative/total*100:.2f}%)\")\n",
    "print(f\"  - Valeurs aberrantes (>1000): {outliers:,} ({outliers/total*100:.2f}%)\")\n",
    "print(f\"  - Doublons: {duplicates:,} ({duplicates/total*100:.2f}%)\")\n",
    "print(f\"  - Formats de dates multiples: 4 formats differents detectes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer la session Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
